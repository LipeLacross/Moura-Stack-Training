# Moura-Stack-Training

## ğŸŒ [English Version of README](README_EN.md)

# Moura Stack â€“ FastAPI + Streamlit + Power BI + ETL (Prefect/Spark) + ML/Stats + Postgres + dbt

Projeto-portfÃ³lio para uma vaga de **EstÃ¡gio/Dev no Grupo Moura**, demonstrando ponta a ponta: **APIs** em FastAPI, **dashboard** em Streamlit, **SQL avanÃ§ado** (consultas, trigger e procedure em PostgreSQL), **embed de Power BI**, **ETL** com Prefect e job **PySpark** (Big Data), **camada Gold** (Parquet/CSV), modelos **dbt** (silver/gold), anÃ¡lises com **Pandas/NumPy/SciPy/Statsmodels**, e um **modelo de regressÃ£o** com Scikit-learn. Inclui endpoints para automaÃ§Ã£o (**/etl/run**) e exportaÃ§Ã£o da camada Gold (**/gold/export**), testes, lint e Docker.

---

## ğŸ”¨ Funcionalidades do Projeto
- **Backend (FastAPI)**  
  - Rotas: `/health`, `/metrics/sales`, `/metrics/summary`, `/stats/pearson`, `/stats/ols`, `/ml/train`, `/ml/predict`, `/etl/run`, `/gold/export`.  
  - **PostgreSQL**: conexÃ£o via SQLAlchemy; **tabela `sales`** com **trigger** (`set_total`) e **procedure** (`upsert_product_revenue`) em `sql/01_init.sql`.
- **Frontend (Streamlit)**  
  - Tabela de amostra, KPIs, **grÃ¡ficos Plotly** (interativo) e **Seaborn/Matplotlib** (estÃ¡tico).  
  - Embed de **Power BI** via `POWER_BI_EMBED_URL`.  
  - BotÃµes para rodar **Pearson/OLS**, **treinar/predizer** (Scikit-learn), disparar **ETL** e **export Gold**.
- **ETL & Big Data**  
  - **Prefect**: `app/etl/flow_etl.py` gera **Parquet** (camada Gold).  
  - **PySpark**: `app/etl/spark_job.py` (opcional) processa CSV â†’ Parquet com cÃ¡lculo de `total`.
- **Analytics & ML**  
  - **SciPy** (correlaÃ§Ã£o Pearson), **Statsmodels** (OLS), **Scikit-learn** (RegressÃ£o Linear).  
  - **OpenPyXL** para export Excel a partir do Streamlit.
- **dbt (MÃ­nimo ViÃ¡vel)**  
  - `stg_sales` (silver) e `fct_sales` (gold) em `dbt/models/`.  
- **Operacional**  
  - Testes `pytest`, lint `ruff`, formatador `black`, **Dockerfile** e **docker-compose**.

---

### ğŸ“¸ Exemplo Visual do Projeto
<div align="center">
  <img src="docs/screenshot-dashboard-1.png" alt="Streamlit Dashboard - KPIs e Tabela" width="80%" style="margin: 16px 0; border-radius: 10px;">
  <img src="docs/screenshot-dashboard-2.png" alt="GrÃ¡ficos Plotly e Seaborn" width="80%" style="margin: 16px 0; border-radius: 10px;">
</div>

> Dica: substitua os caminhos das imagens acima por capturas reais do seu ambiente.

---

## âœ”ï¸ TÃ©cnicas e Tecnologias Utilizadas
- **Linguagem:** Python 3.11+  
- **Backend:** FastAPI, Pydantic, Uvicorn, SQLAlchemy  
- **Banco de Dados:** PostgreSQL (psycopg2) â€” **consultas, trigger e procedure**  
- **Frontend/BI:** Streamlit, **Power BI embed**  
- **Dados/AnÃ¡lises:** Pandas, NumPy, Plotly, Matplotlib, Seaborn, SciPy, Statsmodels  
- **ML:** Scikit-learn (regressÃ£o linear)  
- **ETL/Big Data:** Prefect, PySpark, Parquet (PyArrow)  
- **Modelagem de Dados:** dbt (silver/gold)  
- **Dev/Qualidade:** pytest, requests, Ruff, Black, Docker

---

## ğŸ“ Estrutura do Projeto
```

moura-stack/
â”œâ”€ app/
â”‚  â”œâ”€ backend/
â”‚  â”‚  â”œâ”€ main.py               # FastAPI app / CORS / include\_routers
â”‚  â”‚  â”œâ”€ db.py                 # engine, SessionLocal, ping()
â”‚  â”‚  â”œâ”€ models.py             # Schemas Pydantic (SalesRecord, Summary, ML, Stats)
â”‚  â”‚  â””â”€ routers/
â”‚  â”‚     â”œâ”€ health.py          # /health
â”‚  â”‚     â”œâ”€ metrics.py         # /metrics/sales, /metrics/summary
â”‚  â”‚     â”œâ”€ stats.py           # /stats/pearson, /stats/ols
â”‚  â”‚     â”œâ”€ ml.py              # /ml/train, /ml/predict
â”‚  â”‚     â”œâ”€ etl.py             # /etl/run (webhook p/ Automate)
â”‚  â”‚     â””â”€ gold.py            # /gold/export (Parquet/CSV)
â”‚  â”œâ”€ frontend/
â”‚  â”‚  â””â”€ streamlit\_app.py      # Dashboard, grÃ¡ficos, aÃ§Ãµes
â”‚  â””â”€ etl/
â”‚     â”œâ”€ flow\_etl.py           # Prefect flow (CSV/Postgres â†’ Parquet)
â”‚     â””â”€ spark\_job.py          # ETL PySpark opcional
â”œâ”€ data/
â”‚  â”œâ”€ sample\_sales.csv
â”‚  â””â”€ processed/               # saÃ­da Parquet/CSV (Gold)
â”œâ”€ dbt/
â”‚  â”œâ”€ dbt\_project.yml
â”‚  â””â”€ models/
â”‚     â”œâ”€ schema.yml
â”‚     â”œâ”€ stg\_sales.sql         # silver
â”‚     â””â”€ fct\_sales.sql         # gold
â”œâ”€ sql/
â”‚  â””â”€ 01\_init.sql              # tabela, trigger e procedure (Postgres)
â”œâ”€ tests/
â”‚  â”œâ”€ test\_api.py
â”‚  â””â”€ test\_stats\_ml.py
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â”œâ”€ pyproject.toml
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â””â”€ README.md

````

---

## ğŸ› ï¸ Abrir e rodar o projeto

### 1) PrÃ©-requisitos
- **Python 3.11+**
- **PostgreSQL** (se for usar DB real)
- (Opcional) **Docker** e **Docker Compose**
- (Opcional p/ Spark) **Java 17**

### 2) Clonar e configurar
```bash
git clone <URL_DO_REPOSITORIO>
cd moura-stack
python -m venv .venv
. .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
````

Edite `.env`:

* Para **CSV** (default): mantenha `ETL_SOURCE=csv`.
* Para **Postgres**:

  * `DATABASE_URL=postgresql+psycopg2://user:pass@host:5432/dbname`
  * `ETL_SOURCE=postgres`

### 3) Banco (PostgreSQL)

Execute o script:

```bash
# Via psql ou cliente de sua preferÃªncia
# psql "postgresql://user:pass@host:5432/dbname" -f sql/01_init.sql
```

Isso cria **sales**, a **trigger** `set_total` e a **procedure** `upsert_product_revenue`, alÃ©m de amostras.

### 4) Backend (FastAPI)

```bash
uvicorn app.backend.main:app --reload --host 0.0.0.0 --port 8000
# http://localhost:8000/docs
```

### 5) Frontend (Streamlit)

```bash
export BACKEND_BASE_URL=http://localhost:8000
streamlit run app/frontend/streamlit_app.py --server.port 8501
# http://localhost:8501
```

### 6) ETL

* **Prefect (local):**

  ```bash
  python app/etl/flow_etl.py
  ```
* **Webhook p/ Power Automate:**

  ```bash
  curl -X POST http://localhost:8000/etl/run
  ```

### 7) Export Gold

```bash
curl -X POST http://localhost:8000/gold/export
# Gera Parquet/CSV em data/processed/
```

### 8) Spark (opcional)

```bash
python app/etl/spark_job.py
```

### 9) Testes e Qualidade

```bash
pytest
ruff check .
black .
```

---

## ğŸŒ Deploy

### OpÃ§Ã£o A â€” Docker local

```bash
docker compose up --build
```

* Sobe **API** e **Streamlit** nos ports definidos no `.env`.
* Para usar **Postgres externo**, aponte `DATABASE_URL` no `.env`.

### OpÃ§Ã£o B â€” Nuvem (resumo)

* **API**: conteinerize (Dockerfile jÃ¡ pronto) e suba em um serviÃ§o gerenciado (Railway, Render, Fly.io, Azure Web Apps, AWS ECS/Fargate).
* **Streamlit**: rodar como serviÃ§o separado (mesma imagem) ou migrar para framework web do seu stack.
* **Banco**: PostgreSQL gerenciado (Azure/AWS/GCP).
* **Power BI**: publicar o relatÃ³rio e definir `POWER_BI_EMBED_URL`. Para **Embedded**, crie um endpoint de token (nÃ£o incluso).
* **AutomaÃ§Ã£o**: conecte o **Power Automate** ao webhook `/etl/run`.
* **dbt**: aponte `profile` para o Postgres da nuvem e rode `dbt run`.

---

## âœ… Match com a vaga (resumo)

* **SQL (consultas, triggers, procedures)** â†’ `sql/01_init.sql` + leitura via **SQLAlchemy**.
* **Dashboards/RelatÃ³rios** â†’ **Streamlit** + **Plotly/Matplotlib/Seaborn** + export **Excel**.
* **Power BI Service** â†’ **embed** + **/gold/export** (Parquet/CSV).
* **AutomaÃ§Ã£o** â†’ webhook **/etl/run** acionÃ¡vel pelo **Power Automate**.
* **Python (bibliotecas)** â†’ Pandas, NumPy, SciPy, Statsmodels, Scikit-learn.
* **Big Data** â†’ **PySpark** gerando **Parquet**.
* **dbt (conceitos)** â†’ `stg_sales` (silver) e `fct_sales` (gold).
