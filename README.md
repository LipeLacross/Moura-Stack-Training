## üåê [English Version of README](README_EN.md)

# Moura-Stack-Training

Projeto desenvolvido para demonstrar compet√™ncias t√©cnicas(Moura) em **APIs (FastAPI)**, **dashboard em Jinja**, **SQL avan√ßado em PostgreSQL** (consultas, trigger, procedure), **ETL com Prefect**, **Big Data com PySpark**, **exporta√ß√µes em Parquet/CSV e Excel**, al√©m de **modelagem anal√≠tica com dbt**.  
O projeto tamb√©m integra **Power BI embed**, estat√≠stica (Pearson/OLS) e machine learning (regress√£o linear com Scikit-learn).  

---

## üî® Funcionalidades do Projeto
- **API REST (FastAPI)**  
  Endpoints: `/health`, `/metrics/sales`, `/metrics/summary`, `/stats/pearson`, `/stats/ols`, `/ml/train`, `/ml/predict`, `/etl/run`, `/gold/export`, `/export/excel`, `/spark/run`.
- **Dashboard Jinja** com KPIs, gr√°ficos interativos (Plotly) e est√°ticos (Matplotlib/Seaborn).
- **Banco de Dados PostgreSQL**: tabela `sales`, trigger `set_total` e procedure `upsert_product_revenue`.
- **ETL com Prefect**: fluxo para gerar Parquet/CSV (camada Gold).
- **PySpark**: job para processamento em larga escala.
- **Export Excel**: endpoint usando OpenPyXL.
- **dbt**: modelos `stg_sales` (silver) e `fct_sales` (gold).
- **Infraestrutura**: Docker/Docker Compose prontos para uso.

---

### üì∏ Exemplo Visual do Projeto
	
<div align="center">
  <img src="docs/screenshot-dashboard-1.png" alt="Screenshot 2025-07-03 132707" width="80%" style="margin: 16px 0; border-radius: 10px;">
  <img src="docs/screenshot-dashboard-2.png" alt="Screenshot 2025-07-03 130932" width="80%" style="margin: 16px 0; border-radius: 10px;">
</div>

---

## ‚úîÔ∏è T√©cnicas e Tecnologias Utilizadas
- **Linguagem:** Python 3.11+
- **Backend:** FastAPI, Pydantic, Uvicorn, SQLAlchemy
- **Banco de Dados:** PostgreSQL (psycopg2)
- **Frontend/BI:** Jinja + Power BI embed
- **An√°lises:** Pandas, NumPy, Plotly, Matplotlib, Seaborn, Statsmodels
- **ML:** Scikit-learn (Regress√£o Linear)
- **ETL/Big Data:** Prefect, PySpark, Parquet (PyArrow)
- **Modelagem de Dados:** dbt
- **Dev/Qualidade:** Black, Ruff, Docker

---

## üìÅ Estrutura do Projeto
- **app/backend/**
  - `main.py`: instancia FastAPI, configura√ß√µes, dashboard Jinja
  - `db.py`: conex√£o com PostgreSQL via SQLAlchemy
  - `models.py`: schemas Pydantic
  - **routers/**
    - `health.py`: rota `/health`
    - `metrics.py`: m√©tricas `/metrics/sales`, `/metrics/summary`
    - `stats.py`: estat√≠stica `/stats/pearson`, `/stats/ols`
    - `ml.py`: machine learning `/ml/train`, `/ml/predict`
    - `etl.py`: execu√ß√£o `/etl/run`
    - `gold.py`: export `/gold/export`
    - `extras.py`: `/export/excel`, `/spark/run`
    - `flow_etl.py`: fluxo Prefect
    - `spark_job.py`: job PySpark
- **app/templates/**
  - `base.html`: layout base
  - `dashboard.html`: dashboard interativo
- **data/**
  - `sample_sales.csv`: dados de exemplo
- **dbt/**
  - `dbt_project.yml`: configura√ß√£o
  - `stg_sales.sql`: camada silver
  - `fct_sales.sql`: camada gold
- **sql/**
  - `01_init.sql`: tabela, trigger e procedure
- **infraestrutura**
  - `requirements.txt`, `pyproject.toml`
  - `Dockerfile`, `docker-compose.yml`
  - `.env` e `.env.example`

---

## üõ†Ô∏è Abrir e rodar o projeto
Para iniciar o projeto localmente, siga os passos abaixo:

1. **Pr√©-requisitos**
   - Python 3.11+
   - PostgreSQL instalado
   - (Opcional) Docker/Docker Compose
   - (Opcional) Java 17 para PySpark

2. **Clone o Reposit√≥rio**
   ```bash
   git clone <URL_DO_REPOSITORIO>
   cd moura-stack-training
````

3. **Configura√ß√£o do ambiente**

   ```bash
   python -m venv .venv
   . .venv/bin/activate   # Windows: .venv\Scripts\activate
   pip install -r requirements.txt
   cp .env.example .env
   ```

   * Configure no `.env`:

     * `DATABASE_URL=postgresql+psycopg2://user:pass@host:5432/dbname`
     * `ETL_SOURCE=csv` ou `postgres`
     * `POWER_BI_EMBED_URL=...`

4. **Rodar o Banco**

   ```bash
   psql "postgresql://user:pass@host:5432/dbname" -f sql/01_init.sql
   ```

5. **Iniciar o Backend**

   ```bash
   uvicorn app.backend.main:app --reload --port 8000
   # http://localhost:8000/docs
   ```

6. **Dashboard Jinja**

   ```
   http://localhost:8000/dashboard
   ```

7. **ETL & Exporta√ß√µes**

   ```bash
   curl -X POST http://localhost:8000/etl/run
   curl -X POST http://localhost:8000/gold/export
   curl -X POST http://localhost:8000/export/excel
   ```

8. **Rodar Spark (opcional)**

   ```bash
   python app/backend/spark_job.py
   ```

---

## üåê Deploy

* **Docker local**

  ```bash
  docker compose up --build
  ```

* **Nuvem**

  * **API**: deploy via Docker em servi√ßos como Railway, Render, Fly.io ou AWS ECS.
  * **Banco**: use PostgreSQL gerenciado (RDS, CloudSQL, Azure).
  * **Power BI**: configure `POWER_BI_EMBED_URL`.
  * **dbt**: aponte para o Postgres da nuvem e rode `dbt run`.

```

